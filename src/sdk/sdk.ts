/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import { call } from "../funcs/call.js";
import { stream } from "../funcs/stream.js";
import { ClientSDK, RequestOptions } from "../lib/sdks.js";
import * as models from "../models/index.js";
import * as operations from "../models/operations/index.js";
import { unwrapAsync } from "../types/fp.js";
import { Analytics } from "./analytics.js";
import { Datasets } from "./datasets.js";
import { Embeddings } from "./embeddings.js";
import { Functions } from "./functions.js";
import { Knowledge } from "./knowledge.js";
import { LanguageModels } from "./languagemodels.js";
import { Models } from "./models.js";
import { Openai } from "./openai.js";
import { SpanMetrics } from "./spanmetrics.js";
import { Spans } from "./spans.js";
import { Traces } from "./traces.js";

export class Opper extends ClientSDK {
  private _knowledge?: Knowledge;
  get knowledge(): Knowledge {
    return (this._knowledge ??= new Knowledge(this._options));
  }

  private _traces?: Traces;
  get traces(): Traces {
    return (this._traces ??= new Traces(this._options));
  }

  private _spans?: Spans;
  get spans(): Spans {
    return (this._spans ??= new Spans(this._options));
  }

  private _spanMetrics?: SpanMetrics;
  get spanMetrics(): SpanMetrics {
    return (this._spanMetrics ??= new SpanMetrics(this._options));
  }

  private _datasets?: Datasets;
  get datasets(): Datasets {
    return (this._datasets ??= new Datasets(this._options));
  }

  private _functions?: Functions;
  get functions(): Functions {
    return (this._functions ??= new Functions(this._options));
  }

  private _embeddings?: Embeddings;
  get embeddings(): Embeddings {
    return (this._embeddings ??= new Embeddings(this._options));
  }

  private _languageModels?: LanguageModels;
  get languageModels(): LanguageModels {
    return (this._languageModels ??= new LanguageModels(this._options));
  }

  private _models?: Models;
  get models(): Models {
    return (this._models ??= new Models(this._options));
  }

  private _openai?: Openai;
  get openai(): Openai {
    return (this._openai ??= new Openai(this._options));
  }

  private _analytics?: Analytics;
  get analytics(): Analytics {
    return (this._analytics ??= new Analytics(this._options));
  }

  /**
   * Function Call
   *
   * @remarks
   * The Call endpoint is a simple interface to issue a task to an LLM.
   * It is a declarative interface with input and output schemas that supports text, image, audio inputs and outputs and is highly model agnostic.
   */
  async call(
    request: models.AppApiPublicV2FunctionCallCallFunctionRequest,
    options?: RequestOptions,
  ): Promise<models.AppApiPublicV2FunctionCallCallFunctionResponse> {
    return unwrapAsync(call(
      this,
      request,
      options,
    ));
  }

  /**
   * Function Stream
   *
   * @remarks
   * Stream a function call execution in real-time using Server-Sent Events (SSE).
   *
   * This endpoint provides continuous streaming of function execution results, supporting both
   * unstructured text streaming and structured JSON streaming with precise field tracking.
   *
   * ## Streaming Modes
   *
   * **Text Mode (no output_schema):**
   * - Streams incremental text content via the `delta` field
   * - `chunk_type` will be "text"
   * - Best for conversational AI, creative writing, open-ended responses
   *
   * **Structured Mode (with output_schema):**
   * - Streams structured JSON with precise field tracking via `json_path`
   * - `chunk_type` will be "json"
   * - Enables real-time UI updates by showing which schema field is being populated
   * - Perfect for forms, dashboards, structured data display
   *
   * ## JSON Path Feature
   *
   * When using `output_schema`, each streaming chunk includes a `json_path` field showing exactly
   * which field in your schema is being populated:
   *
   * - `response.summary` → Top-level string field
   * - `response.people[0].name` → Name of first person in array
   * - `response.people[1].role` → Role of second person
   * - `response.metadata.created_at` → Nested object field
   *
   * This enables precise UI updates where you can route streaming content to specific components
   * based on the path, creating responsive real-time interfaces.
   *
   * ## Response Structure
   *
   * Each Server-Sent Event contains:
   * - `id`: Optional event identifier
   * - `event`: Optional event type (typically "message")
   * - `data`: StreamingChunk with the actual streaming content
   * - `retry`: Optional retry interval for reconnection
   *
   * The StreamingChunk data payload varies by mode:
   *
   * **Text Mode:**
   * - `delta`: Incremental text content
   * - `span_id`: Execution span ID (first chunk)
   * - `chunk_type`: "text"
   *
   * **Structured Mode:**
   * - `delta`: Actual field values being streamed
   * - `json_path`: Dot-notation path to current field
   * - `span_id`: Execution span ID (first chunk)
   * - `chunk_type`: "json"
   *
   * ## Examples
   *
   * Text streaming events:
   * ```
   * data: {"span_id": "123e4567-e89b-12d3-a456-426614174000"}
   * data: {"delta": "Hello", "chunk_type": "text"}
   * data: {"delta": " world", "chunk_type": "text"}
   * ```
   *
   * Structured streaming events:
   * ```
   * data: {"span_id": "123e4567-e89b-12d3-a456-426614174000"}
   * data: {"delta": "John", "json_path": "response.name", "chunk_type": "json"}
   * data: {"delta": " Doe", "json_path": "response.name", "chunk_type": "json"}
   * data: {"delta": "Engineer", "json_path": "response.role", "chunk_type": "json"}
   * ```
   */
  async stream(
    request: models.AppApiPublicV2FunctionCallCallFunctionRequest,
    options?: RequestOptions,
  ): Promise<operations.FunctionStreamCallStreamPostResponse> {
    return unwrapAsync(stream(
      this,
      request,
      options,
    ));
  }
}
